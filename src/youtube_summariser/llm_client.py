"""LLM Client abstraction for OpenAI and Anthropic."""

import logging
import os
from importlib import resources
from typing import Iterator, Optional

import yaml

logger = logging.getLogger(__name__)


def load_config() -> dict:
    """Load configuration from bundled config.yaml."""
    try:
        config_file = resources.files(__package__) / "config.yaml"
        with config_file.open("r") as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        # Return default config if file not found
        return {
            "provider": "google",
            "openai": {"model": "gpt-5.2", "max_tokens": 3000},
            "anthropic": {"model": "claude-sonnet-4-5-20250929", "max_tokens": 3000},
            "google": {"model": "gemini-3-flash-preview", "max_tokens": 3000},
        }
    except yaml.YAMLError as e:
        raise ValueError(f"Invalid YAML in configuration file: {e}")


class LLMClient:
    """Unified LLM client supporting OpenAI and Anthropic."""

    def __init__(self, config: Optional[dict] = None, provider: Optional[str] = None):
        """
        Initialize the LLM client.

        Args:
            config: Optional configuration dict. If not provided, loads from config.yaml.
            provider: Optional provider override ('openai' or 'anthropic').
        """
        self.config = config or load_config()
        self.provider = provider or self.config.get("provider", "google")
        self._client = None
        self._init_client()

    def _init_client(self):
        """Initialize the appropriate client based on provider."""
        openai_key = os.environ.get("OPENAI_API_KEY")
        anthropic_key = os.environ.get("ANTHROPIC_API_KEY")
        google_key = os.environ.get("GEMINI_API_KEY")

        # Check if neither key is available
        if not any([openai_key, anthropic_key, google_key]):
            raise ValueError("No API keys found. Set OPENAI_API_KEY, ANTHROPIC_API_KEY, or GEMINI_API_KEY.")

        if self.provider == "openai":
            from openai import OpenAI

            if not openai_key:
                raise ValueError(
                    "OPENAI_API_KEY environment variable is not set. "
                    "Use --provider anthropic to use your configured ANTHROPIC_API_KEY instead."
                )
            self._client = OpenAI(api_key=openai_key)
        elif self.provider == "anthropic":
            import anthropic

            if not anthropic_key:
                raise ValueError(
                    "ANTHROPIC_API_KEY environment variable is not set. "
                    "Use --provider openai to use your configured OPENAI_API_KEY instead."
                )
            self._client = anthropic.Anthropic(api_key=anthropic_key)
        elif self.provider == "google":
            from google import genai
            if not google_key: raise ValueError("GEMINI_API_KEY not set.")
            self._client = genai.Client(api_key=google_key)
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")

    def get_model(self) -> str:
        provider_config = self.config.get(self.provider, {})
        defaults = {
            "openai": "gpt-5.2",
            "anthropic": "claude-sonnet-4-5-20250929",
            "google": "gemini-3-flash-preview"
        }
        return provider_config.get("model", defaults.get(self.provider))

    def get_max_tokens(self) -> int:
        """Get max tokens for the current provider."""
        provider_config = self.config.get(self.provider, {})
        return provider_config.get("max_tokens", 3000)

    def chat(self, system_prompt: str, user_message: str) -> str:
        """
        Send a chat request to the LLM.

        Args:
            system_prompt: The system prompt
            user_message: The user message

        Returns:
            The assistant's response text
        """
        if self.provider == "openai":
            return self._chat_openai(system_prompt, user_message)
        elif self.provider == "anthropic":
            return self._chat_anthropic(system_prompt, user_message)
        elif self.provider == "google":
            return self._chat_google(system_prompt, user_message)
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")

    def stream_chat(self, system_prompt: str, user_message: str) -> Iterator[str]:
        """
        Send a streaming chat request to the LLM.

        Args:
            system_prompt: The system prompt
            user_message: The user message

        Yields:
            Text chunks as they are generated by the LLM
        """
        if self.provider == "openai":
            yield from self._stream_chat_openai(system_prompt, user_message)
        elif self.provider == "anthropic":
            yield from self._stream_chat_anthropic(system_prompt, user_message)
        elif self.provider == "google":
            yield from self._stream_chat_google(system_prompt, user_message)
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")

    def _chat_openai(self, system_prompt: str, user_message: str) -> str:
        """Send chat request to OpenAI."""
        response = self._client.chat.completions.create(
            model=self.get_model(),
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message},
            ],
            max_completion_tokens=self.get_max_tokens(),
        )
        return response.choices[0].message.content

    def _chat_anthropic(self, system_prompt: str, user_message: str) -> str:
        """Send chat request to Anthropic."""
        response = self._client.messages.create(
            model=self.get_model(),
            max_tokens=self.get_max_tokens(),
            system=system_prompt,
            messages=[{"role": "user", "content": user_message}],
        )
        return response.content[0].text

    def _chat_google(self, system_prompt: str, user_message: str) -> str:
        from google.genai import types
        response = self._client.models.generate_content(
            model=self.get_model(),
            contents=user_message,
            config=types.GenerateContentConfig(
                system_instruction=system_prompt,
                max_output_tokens=self.get_max_tokens(),
            ),
        )
        return response.text


    def _stream_chat_openai(self, system_prompt: str, user_message: str) -> Iterator[str]:
        """Send streaming chat request to OpenAI."""
        stream = self._client.chat.completions.create(
            model=self.get_model(),
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message},
            ],
            max_completion_tokens=self.get_max_tokens(),
            stream=True,
        )
        for chunk in stream:
            if chunk.choices and len(chunk.choices) > 0:
                delta = chunk.choices[0].delta
                if delta.content:
                    yield delta.content

    def _stream_chat_anthropic(self, system_prompt: str, user_message: str) -> Iterator[str]:
        """Send streaming chat request to Anthropic."""
        with self._client.messages.stream(
            model=self.get_model(),
            max_tokens=self.get_max_tokens(),
            system=system_prompt,
            messages=[{"role": "user", "content": user_message}],
        ) as stream:
            for text in stream.text_stream:
                yield text


    def _stream_chat_google(self, system_prompt: str, user_message: str) -> Iterator[str]:
        from google.genai import types
        response = self._client.models.generate_content_stream(
            model=self.get_model(),
            contents=user_message,
            config=types.GenerateContentConfig(
                system_instruction=system_prompt,
                max_output_tokens=self.get_max_tokens(),
            ),
        )
        for chunk in response:
            yield chunk.text